{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9994a751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Error parsing dependencies of safetensors: [Errno 2] No such file or directory: 'c:\\\\users\\\\lenovo\\\\documents\\\\programming\\\\artificial intelligence\\\\deep learning\\\\.dl-tf\\\\lib\\\\site-packages\\\\safetensors-0.6.2.dist-info\\\\METADATA'\n",
      "WARNING: Error parsing dependencies of transformers: [Errno 2] No such file or directory: 'c:\\\\users\\\\lenovo\\\\documents\\\\programming\\\\artificial intelligence\\\\deep learning\\\\.dl-tf\\\\lib\\\\site-packages\\\\transformers-4.57.1.dist-info\\\\METADATA'\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\lenovo\\documents\\programming\\artificial intelligence\\deep learning\\.dl-tf\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\LENOVO\\Documents\\programming\\artificial intelligence\\deep learning\\.dl-tf\\Lib\\site-packages\\~afetensors'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"transformers>=4.44\" accelerate peft safetensors\n",
    "!pip -q install -U bitsandbytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40607f2",
   "metadata": {},
   "source": [
    "## **Dataset + Data Preparation**\n",
    "**Load Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73772b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ JSON berhasil dibaca\n",
      "Jumlah data : 463\n",
      "Categories  : ['Alur', 'Beasiswa', 'Biaya', 'PMB_Umum', 'Pembayaran', 'Prodi', 'Profesi', 'S1', 'S2', 'S3', 'Umum']\n",
      "Contoh data: {'category': 'S1', 'instruction': 'Jawablah pertanyaan berikut berdasarkan informasi resmi PMB Universitas Atma Jaya Yogyakarta.', 'input': 'Siapa saja yang dapat mendaftar Program Sarjana (S1) di Universitas Atma Jaya Yogyakarta melalui Program Nilai Ijazah?', 'output': 'Program Nilai Ijazah terbuka bagi siswa SMA/SMK yang telah menyelesaikan studi (Lulusan tahun 2026 dan sebelumnya) yang penerimaannya didasarkan pada nilai ijazah.'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# GANTI sesuai nama file di explorer VSCode kamu\n",
    "file_path = \"dataset_pmb_uajy.json\"\n",
    "\n",
    "try:\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Validasi tipe data\n",
    "    if not isinstance(data, list):\n",
    "        raise ValueError(\"JSON harus berupa LIST of objects\")\n",
    "\n",
    "    # Validasi struktur field\n",
    "    required = {\"category\", \"instruction\", \"input\", \"output\"}\n",
    "    for i, ex in enumerate(data[:20]):  # cek 20 pertama\n",
    "        missing = required - set(ex.keys())\n",
    "        if missing:\n",
    "            raise ValueError(f\"Index {i} missing keys: {missing}\")\n",
    "\n",
    "    categories = sorted({d[\"category\"] for d in data})\n",
    "\n",
    "    print(\"✅ JSON berhasil dibaca\")\n",
    "    print(\"Jumlah data :\", len(data))\n",
    "    print(\"Categories  :\", categories)\n",
    "    print(\"Contoh data:\", data[0])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ File '{file_path}' tidak ditemukan. Pastikan nama file & lokasi benar.\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"❌ Format JSON tidak valid. Cek koma, kurung, dll.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Error lain:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5effc472",
   "metadata": {},
   "source": [
    "**Split train/val/test stratified by category**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0021fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370 46 47\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labels = [d[\"category\"] for d in data]\n",
    "\n",
    "train_data, temp_data = train_test_split(\n",
    "    data, test_size=0.2, random_state=42, shuffle=True, stratify=labels\n",
    ")\n",
    "\n",
    "temp_labels = [d[\"category\"] for d in temp_data]\n",
    "val_data, test_data = train_test_split(\n",
    "    temp_data, test_size=0.5, random_state=42, shuffle=True, stratify=temp_labels\n",
    ")\n",
    "\n",
    "print(len(train_data), len(val_data), len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc21ffc",
   "metadata": {},
   "source": [
    "**Templating**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc7c744",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prompt(category: str, instruction: str, inp: str) -> str:\n",
    "    category = (category or \"\").strip()\n",
    "    instruction = (instruction or \"\").strip()\n",
    "    inp = (inp or \"\").strip()\n",
    "\n",
    "    header = f\"### Category: {category}\\n### Instruction:\\n{instruction}\\n\\n\"\n",
    "    if inp:\n",
    "        return header + f\"### Input:\\n{inp}\\n\\n### Response:\\n\"\n",
    "    else:\n",
    "        return header + \"### Response:\\n\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e73d4",
   "metadata": {},
   "source": [
    "## **Tokenization & Prompt Formatting**\n",
    "**Tokenization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a64961",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model_id = \"Sahabat-AI/gemma2-9b-cpt-sahabatai-v1-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "MAX_LEN = 1024\n",
    "\n",
    "class InstructionDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len=1024):\n",
    "        self.data = data\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        ex = self.data[idx]\n",
    "\n",
    "        prompt = format_prompt(\n",
    "            ex.get(\"category\", \"\"),\n",
    "            ex.get(\"instruction\", \"\"),\n",
    "            ex.get(\"input\", \"\")\n",
    "        )\n",
    "        answer = (ex.get(\"output\", \"\") or \"\").strip()\n",
    "\n",
    "        full_text = prompt + answer + self.tok.eos_token\n",
    "\n",
    "        enc = self.tok(\n",
    "            full_text,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "        )\n",
    "\n",
    "        input_ids = enc[\"input_ids\"]\n",
    "        attention_mask = enc[\"attention_mask\"]\n",
    "\n",
    "        # token prompt untuk masking label\n",
    "        prompt_ids = self.tok(\n",
    "            prompt,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        labels = input_ids.copy()\n",
    "        # mask prompt tokens supaya loss fokus pada output\n",
    "        for i in range(min(len(prompt_ids), len(labels))):\n",
    "            labels[i] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "            \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long),\n",
    "            \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "train_ds = InstructionDataset(train_data, tokenizer, MAX_LEN)\n",
    "val_ds   = InstructionDataset(val_data, tokenizer, MAX_LEN)\n",
    "test_ds  = InstructionDataset(test_data, tokenizer, MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9eb47a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apa itu Program Tanpa Tes di UAJY?\n",
      "\n",
      "Program Tanpa Tes di UAJY adalah program yang memungkinkan calon mahasiswa untuk masuk ke UAJY tanpa harus mengikuti tes masuk. Program ini sangat cocok bagi mereka yang ingin masuk ke UAJY tanpa harus melalui proses tes yang panjang dan melelahkan.\n",
      "\n",
      "Bagaimana Cara Mendaftar Program Tanpa Tes di UAJY?\n",
      "\n",
      "Untuk mendaftar Program Tanpa Tes di UAJY, calon mahasiswa harus memenuhi beberapa persyaratan yang telah ditentukan oleh pihak UAJY.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Apa itu Program Tanpa Tes di UAJY?\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=100\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8339e010",
   "metadata": {},
   "source": [
    "**Collate function (padding batch)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12f6420",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = pad_sequence([x[\"input_ids\"] for x in batch], batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    attention_mask = pad_sequence([x[\"attention_mask\"] for x in batch], batch_first=True, padding_value=0)\n",
    "    labels = pad_sequence([x[\"labels\"] for x in batch], batch_first=True, padding_value=-100)\n",
    "\n",
    "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cedb92e",
   "metadata": {},
   "source": [
    "**Test DataLoader + sanity check decode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84d8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "batch = next(iter(train_loader))\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "\n",
    "# lihat 1 contoh text (biar yakin template bener)\n",
    "sample = train_data[0]\n",
    "print(\"\\n--- Preview ---\")\n",
    "print(format_prompt(sample[\"category\"], sample[\"instruction\"], sample.get(\"input\",\"\")) + sample[\"output\"][:200])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (DL - TensorFlow)",
   "language": "python",
   "name": "dl-tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
